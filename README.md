# ğŸš¢ Titanic Survival Prediction

## ğŸ¯ Má»¥c TiÃªu Dá»± Ãn

Dá»± Ã¡n **Titanic Survival Prediction** lÃ  bÃ i toÃ¡n classification kinh Ä‘iá»ƒn trÃªn Kaggle. Báº¡n sáº½ dá»± Ä‘oÃ¡n hÃ nh khÃ¡ch nÃ o cÃ³ kháº£ nÄƒng sá»‘ng sÃ³t sau tháº£m há»a Titanic dá»±a trÃªn thÃ´ng tin cÃ¡ nhÃ¢n, vÃ©, vÃ  cabin. ÄÃ¢y lÃ  dá»± Ã¡n hoÃ n háº£o Ä‘á»ƒ há»c feature engineering vÃ  classification techniques.

## ğŸ“ Kiáº¿n Thá»©c Sáº½ Há»c ÄÆ°á»£c

### ğŸ“š Core ML Concepts
- **Binary Classification**: PhÃ¢n loáº¡i nhá»‹ phÃ¢n
- **Feature Engineering**: Táº¡o Ä‘áº·c trÆ°ng tá»« dá»¯ liá»‡u thÃ´
- **Data Preprocessing**: Xá»­ lÃ½ dá»¯ liá»‡u thiáº¿u, outliers
- **Model Selection**: Chá»n mÃ´ hÃ¬nh phÃ¹ há»£p
- **Hyperparameter Tuning**: Tá»‘i Æ°u tham sá»‘

### ğŸ”§ Techniques & Algorithms
- **Logistic Regression**: Há»“i quy logistic
- **Random Forest**: Rá»«ng ngáº«u nhiÃªn
- **Gradient Boosting**: XGBoost, LightGBM
- **Support Vector Machine**: SVM
- **Neural Networks**: MLP

### ğŸ“Š Data Science Skills
- **Exploratory Data Analysis**: PhÃ¢n tÃ­ch khÃ¡m phÃ¡
- **Feature Selection**: Lá»±a chá»n Ä‘áº·c trÆ°ng
- **Cross-validation**: Kiá»ƒm tra chÃ©o
- **Ensemble Methods**: PhÆ°Æ¡ng phÃ¡p káº¿t há»£p

## ğŸ“ Cáº¥u TrÃºc Dá»± Ãn

```
04-Titanic-Survival/
â”œâ”€â”€ README.md
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01-eda-analysis.ipynb
â”‚   â”œâ”€â”€ 02-feature-engineering.ipynb
â”‚   â”œâ”€â”€ 03-model-training.ipynb
â”‚   â”œâ”€â”€ 04-hyperparameter-tuning.ipynb
â”‚   â”œâ”€â”€ 05-ensemble-methods.ipynb
â”‚   â””â”€â”€ 06-submission-preparation.ipynb
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_preprocessing.py
â”‚   â”œâ”€â”€ feature_engineering.py
â”‚   â”œâ”€â”€ models.py
â”‚   â”œâ”€â”€ evaluation.py
â”‚   â””â”€â”€ submission.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ train.csv
â”‚   â”‚   â””â”€â”€ test.csv
â”‚   â””â”€â”€ processed/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ trained_models/
â”œâ”€â”€ submissions/
â”œâ”€â”€ requirements.txt
â””â”€â”€ .gitignore
```

## ğŸ“Š Dataset Overview

### ğŸš¢ Titanic Dataset
- **Nguá»“n**: [Kaggle Titanic Competition](https://www.kaggle.com/c/titanic)
- **Training Set**: 891 passengers
- **Test Set**: 418 passengers
- **Target**: Survived (0 = No, 1 = Yes)

### ğŸ” Features
- **PassengerId**: Unique identifier
- **Pclass**: Ticket class (1, 2, 3)
- **Name**: Passenger name
- **Sex**: Gender (male, female)
- **Age**: Age in years
- **SibSp**: Siblings/spouses aboard
- **Parch**: Parents/children aboard
- **Ticket**: Ticket number
- **Fare**: Passenger fare
- **Cabin**: Cabin number
- **Embarked**: Port of embarkation (C, Q, S)

## ğŸš€ CÃ¡ch Báº¯t Äáº§u

### 1. Download Dataset
```bash
# Táº¡o thÆ° má»¥c data
mkdir -p data/raw

# Download tá»« Kaggle (cáº§n API key)
kaggle competitions download -c titanic -p data/raw/
unzip data/raw/titanic.zip -d data/raw/
```

### 2. CÃ i Äáº·t MÃ´i TrÆ°á»ng
```bash
python -m venv titanic_env
source titanic_env/bin/activate
pip install -r requirements.txt
```

### 3. Báº¯t Äáº§u EDA
Má»Ÿ `notebooks/01-eda-analysis.ipynb` Ä‘á»ƒ khÃ¡m phÃ¡ dá»¯ liá»‡u!

## ğŸ“‹ Roadmap Há»c Táº­p

### âœ… Phase 1: Exploratory Data Analysis
- [ ] Load vÃ  khÃ¡m phÃ¡ dataset
- [ ] PhÃ¢n tÃ­ch missing values
- [ ] Visualization survival patterns
- [ ] Correlation analysis
- [ ] Outlier detection

### âœ… Phase 2: Feature Engineering
- [ ] Extract title from Name
- [ ] Create family size features
- [ ] Age grouping
- [ ] Fare binning
- [ ] Cabin deck extraction
- [ ] One-hot encoding

### âœ… Phase 3: Model Training
- [ ] Train-test split
- [ ] Baseline models
- [ ] Logistic Regression
- [ ] Random Forest
- [ ] Gradient Boosting
- [ ] Neural Networks

### âœ… Phase 4: Model Optimization
- [ ] Cross-validation
- [ ] Hyperparameter tuning
- [ ] Feature selection
- [ ] Model comparison
- [ ] Performance analysis

### âœ… Phase 5: Ensemble & Submission
- [ ] Voting classifiers
- [ ] Stacking methods
- [ ] Final model selection
- [ ] Test set predictions
- [ ] Kaggle submission

## ğŸ”§ Feature Engineering Techniques

### ğŸ“ Title Extraction
```python
def extract_title(name):
    """Extract title from passenger name"""
    title = name.split(',')[1].split('.')[0].strip()
    # Group rare titles
    if title in ['Mr']:
        return 'Mr'
    elif title in ['Miss', 'Mlle']:
        return 'Miss'
    elif title in ['Mrs', 'Mme']:
        return 'Mrs'
    elif title in ['Master']:
        return 'Master'
    else:
        return 'Rare'

df['Title'] = df['Name'].apply(extract_title)
```

### ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ Family Features
```python
def create_family_features(df):
    """Create family-related features"""
    # Family size
    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1
    
    # Is alone
    df['IsAlone'] = (df['FamilySize'] == 1).astype(int)
    
    # Family size groups
    df['FamilySizeGroup'] = pd.cut(df['FamilySize'], 
                                  bins=[0, 1, 4, 7, 20], 
                                  labels=['Alone', 'Small', 'Medium', 'Large'])
    
    return df
```

### ğŸ‚ Age Grouping
```python
def create_age_groups(df):
    """Create age groups"""
    df['AgeGroup'] = pd.cut(df['Age'], 
                           bins=[0, 12, 18, 35, 60, 100], 
                           labels=['Child', 'Teen', 'Adult', 'Middle', 'Senior'])
    
    # Fill missing ages with median by title
    for title in df['Title'].unique():
        median_age = df[df['Title'] == title]['Age'].median()
        df.loc[(df['Age'].isna()) & (df['Title'] == title), 'Age'] = median_age
    
    return df
```

## ğŸ¤– Model Implementation

### ğŸ“Š Baseline Models
```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from xgboost import XGBClassifier

# Initialize models
models = {
    'Logistic Regression': LogisticRegression(random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42),
    'SVM': SVC(random_state=42),
    'XGBoost': XGBClassifier(random_state=42)
}

# Train and evaluate
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    results[name] = accuracy
    print(f"{name}: {accuracy:.4f}")
```

### ğŸ¯ Hyperparameter Tuning
```python
from sklearn.model_selection import GridSearchCV

# Random Forest tuning
rf_params = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

rf_grid = GridSearchCV(
    RandomForestClassifier(random_state=42),
    rf_params,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)

rf_grid.fit(X_train, y_train)
print(f"Best RF params: {rf_grid.best_params_}")
print(f"Best RF score: {rf_grid.best_score_:.4f}")
```

## ğŸ“Š Evaluation Metrics

### ğŸ¯ Classification Metrics
```python
from sklearn.metrics import classification_report, confusion_matrix

def evaluate_model(y_true, y_pred, model_name):
    """Comprehensive model evaluation"""
    print(f"\n=== {model_name} Evaluation ===")
    
    # Accuracy
    accuracy = accuracy_score(y_true, y_pred)
    print(f"Accuracy: {accuracy:.4f}")
    
    # Classification report
    print("\nClassification Report:")
    print(classification_report(y_true, y_pred))
    
    # Confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'{model_name} - Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()
    
    return accuracy
```

## ğŸ¯ Ensemble Methods

### ğŸ—³ï¸ Voting Classifier
```python
from sklearn.ensemble import VotingClassifier

# Create ensemble
ensemble = VotingClassifier([
    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),
    ('xgb', XGBClassifier(random_state=42)),
    ('lr', LogisticRegression(random_state=42))
], voting='soft')

# Train ensemble
ensemble.fit(X_train, y_train)
ensemble_pred = ensemble.predict(X_test)
ensemble_accuracy = accuracy_score(y_test, ensemble_pred)
print(f"Ensemble Accuracy: {ensemble_accuracy:.4f}")
```

### ğŸ“š Stacking
```python
from sklearn.ensemble import StackingClassifier

# Create stacking classifier
stacking = StackingClassifier([
    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),
    ('xgb', XGBClassifier(random_state=42))
], 
final_estimator=LogisticRegression(random_state=42),
cv=5)

# Train stacking
stacking.fit(X_train, y_train)
stacking_pred = stacking.predict(X_test)
stacking_accuracy = accuracy_score(y_test, stacking_pred)
print(f"Stacking Accuracy: {stacking_accuracy:.4f}")
```

## ğŸ¯ Expected Results

Sau khi hoÃ n thÃ nh dá»± Ã¡n nÃ y, báº¡n sáº½ cÃ³:

1. **High Accuracy Model**: >80% accuracy trÃªn test set
2. **Feature Engineering Skills**: Táº¡o features tá»« dá»¯ liá»‡u thÃ´
3. **Model Comparison**: So sÃ¡nh nhiá»u algorithms
4. **Ensemble Knowledge**: Káº¿t há»£p models hiá»‡u quáº£
5. **Kaggle Submission**: Submit káº¿t quáº£ lÃªn Kaggle

## ğŸ” Key Insights to Discover

### ğŸ“Š Data Insights
- **Women vÃ  children** cÃ³ tá»· lá»‡ sá»‘ng sÃ³t cao hÆ¡n
- **Higher class passengers** Æ°u tiÃªn trong rescue
- **Family size** áº£nh hÆ°á»Ÿng Ä‘áº¿n survival
- **Age** cÃ³ correlation vá»›i survival

### ğŸ¯ Model Insights
- **Feature engineering** quan trá»ng hÆ¡n algorithm choice
- **Ensemble methods** thÆ°á»ng cho káº¿t quáº£ tá»‘t hÆ¡n
- **Cross-validation** cáº§n thiáº¿t Ä‘á»ƒ trÃ¡nh overfitting
- **Hyperparameter tuning** cáº£i thiá»‡n performance Ä‘Ã¡ng ká»ƒ

## ğŸ“š TÃ i Liá»‡u Tham Kháº£o

- [Kaggle Titanic Competition](https://www.kaggle.com/c/titanic)
- [Feature Engineering Guide](https://www.kaggle.com/learn/feature-engineering)
- [Ensemble Methods](https://scikit-learn.org/stable/modules/ensemble.html)
- [XGBoost Documentation](https://xgboost.readthedocs.io/)

## ğŸ† Next Steps

Sau khi hoÃ n thÃ nh Titanic Survival Prediction, báº¡n cÃ³ thá»ƒ:
- Thá»­ advanced feature engineering
- Chuyá»ƒn sang dá»± Ã¡n 5: Housing Price Predictor
- PhÃ¡t triá»ƒn web app vá»›i Streamlit
- Tham gia Kaggle competitions khÃ¡c

## ğŸ¨ Bonus: Streamlit Dashboard

Táº¡o web app Ä‘á»ƒ demo model:
- Input passenger information
- Predict survival probability
- Show feature importance
- Visualize decision path

---

**Happy Predicting! ğŸš¢**

*HÃ£y báº¯t Ä‘áº§u vá»›i EDA vÃ  khÃ¡m phÃ¡ nhá»¯ng cÃ¢u chuyá»‡n áº©n sau dá»¯ liá»‡u Titanic!*
