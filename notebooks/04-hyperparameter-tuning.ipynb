{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ¯ Titanic Survival Prediction - Hyperparameter Tuning\n",
        "\n",
        "## ğŸ“Š Má»¥c tiÃªu\n",
        "- Tá»‘i Æ°u hÃ³a hyperparameters cho cÃ¡c models tá»‘t nháº¥t\n",
        "- Sá»­ dá»¥ng GridSearchCV vÃ  RandomSearchCV\n",
        "- So sÃ¡nh performance trÆ°á»›c vÃ  sau tuning\n",
        "- Chá»n best parameters cho production\n",
        "\n",
        "## ğŸ“‹ Ná»™i dung\n",
        "1. **Data Preparation**\n",
        "2. **Random Forest Tuning**\n",
        "3. **XGBoost Tuning**\n",
        "4. **Logistic Regression Tuning**\n",
        "5. **SVM Tuning**\n",
        "6. **Model Comparison After Tuning**\n",
        "7. **Best Model Selection**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import xgboost as xgb\n",
        "import joblib\n",
        "import time\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"ğŸ“š Libraries imported successfully!\")\n",
        "print(\"ğŸ¨ Visualization style set!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. ğŸ“¥ Data Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import preprocessing utilities\n",
        "import sys\n",
        "sys.path.append('../src')\n",
        "from data_preprocessing import load_data, preprocess_data, prepare_features, get_feature_columns\n",
        "from models import ModelTrainer\n",
        "\n",
        "# Load and preprocess data\n",
        "train_df, test_df = load_data('../data/raw/train.csv', '../data/raw/test.csv')\n",
        "processed_train_df, processed_test_df, label_encoders = preprocess_data(train_df, test_df)\n",
        "\n",
        "# Prepare features\n",
        "feature_columns = get_feature_columns()\n",
        "X = prepare_features(processed_train_df, feature_columns)\n",
        "y = processed_train_df['Survived']\n",
        "\n",
        "# Split data for hyperparameter tuning\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"ğŸ“Š Training set shape: {X_train.shape}\")\n",
        "print(f\"ğŸ“Š Test set shape: {X_test.shape}\")\n",
        "print(f\"ğŸ“Š Feature columns: {feature_columns}\")\n",
        "\n",
        "# Display feature info\n",
        "print(f\"\\nğŸ” Feature Information:\")\n",
        "print(f\"Features: {list(X.columns)}\")\n",
        "print(f\"Target distribution: {y.value_counts().to_dict()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. ğŸŒ² Random Forest Hyperparameter Tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Random Forest Hyperparameter Tuning\n",
        "print(\"ğŸŒ² Random Forest Hyperparameter Tuning\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Define parameter grid for Random Forest\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [50, 100, 200, 300],\n",
        "    'max_depth': [3, 5, 7, 10, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2', None],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Initialize Random Forest\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Use RandomizedSearchCV for faster search\n",
        "rf_random_search = RandomizedSearchCV(\n",
        "    estimator=rf,\n",
        "    param_distributions=rf_param_grid,\n",
        "    n_iter=50,  # Number of parameter settings sampled\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "print(\"ğŸ” Searching for best Random Forest parameters...\")\n",
        "start_time = time.time()\n",
        "rf_random_search.fit(X_train, y_train)\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"â±ï¸ Search completed in {end_time - start_time:.2f} seconds\")\n",
        "print(f\"ğŸ† Best parameters: {rf_random_search.best_params_}\")\n",
        "print(f\"ğŸ¯ Best cross-validation score: {rf_random_search.best_score_:.4f}\")\n",
        "\n",
        "# Test on holdout set\n",
        "rf_best = rf_random_search.best_estimator_\n",
        "rf_y_pred = rf_best.predict(X_test)\n",
        "rf_accuracy = accuracy_score(y_test, rf_y_pred)\n",
        "print(f\"ğŸ“Š Test accuracy: {rf_accuracy:.4f}\")\n",
        "\n",
        "# Store results\n",
        "rf_results = {\n",
        "    'best_params': rf_random_search.best_params_,\n",
        "    'best_cv_score': rf_random_search.best_score_,\n",
        "    'test_accuracy': rf_accuracy,\n",
        "    'model': rf_best\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. ğŸš€ XGBoost Hyperparameter Tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# XGBoost Hyperparameter Tuning\n",
        "print(\"ğŸš€ XGBoost Hyperparameter Tuning\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Define parameter grid for XGBoost\n",
        "xgb_param_grid = {\n",
        "    'n_estimators': [50, 100, 200, 300],\n",
        "    'max_depth': [3, 4, 5, 6, 7],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
        "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "    'reg_alpha': [0, 0.1, 0.5, 1.0],\n",
        "    'reg_lambda': [0, 0.1, 0.5, 1.0]\n",
        "}\n",
        "\n",
        "# Initialize XGBoost\n",
        "xgb_model = xgb.XGBClassifier(random_state=42, eval_metric='logloss')\n",
        "\n",
        "# Use RandomizedSearchCV for faster search\n",
        "xgb_random_search = RandomizedSearchCV(\n",
        "    estimator=xgb_model,\n",
        "    param_distributions=xgb_param_grid,\n",
        "    n_iter=50,  # Number of parameter settings sampled\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "print(\"ğŸ” Searching for best XGBoost parameters...\")\n",
        "start_time = time.time()\n",
        "xgb_random_search.fit(X_train, y_train)\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"â±ï¸ Search completed in {end_time - start_time:.2f} seconds\")\n",
        "print(f\"ğŸ† Best parameters: {xgb_random_search.best_params_}\")\n",
        "print(f\"ğŸ¯ Best cross-validation score: {xgb_random_search.best_score_:.4f}\")\n",
        "\n",
        "# Test on holdout set\n",
        "xgb_best = xgb_random_search.best_estimator_\n",
        "xgb_y_pred = xgb_best.predict(X_test)\n",
        "xgb_accuracy = accuracy_score(y_test, xgb_y_pred)\n",
        "print(f\"ğŸ“Š Test accuracy: {xgb_accuracy:.4f}\")\n",
        "\n",
        "# Store results\n",
        "xgb_results = {\n",
        "    'best_params': xgb_random_search.best_params_,\n",
        "    'best_cv_score': xgb_random_search.best_score_,\n",
        "    'test_accuracy': xgb_accuracy,\n",
        "    'model': xgb_best\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. ğŸ“ˆ Logistic Regression Hyperparameter Tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Logistic Regression Hyperparameter Tuning\n",
        "print(\"ğŸ“ˆ Logistic Regression Hyperparameter Tuning\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Define parameter grid for Logistic Regression\n",
        "lr_param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
        "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "    'solver': ['liblinear', 'lbfgs', 'saga'],\n",
        "    'max_iter': [100, 500, 1000, 2000]\n",
        "}\n",
        "\n",
        "# Initialize Logistic Regression\n",
        "lr = LogisticRegression(random_state=42)\n",
        "\n",
        "# Use GridSearchCV for Logistic Regression (smaller parameter space)\n",
        "lr_grid_search = GridSearchCV(\n",
        "    estimator=lr,\n",
        "    param_grid=lr_param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "print(\"ğŸ” Searching for best Logistic Regression parameters...\")\n",
        "start_time = time.time()\n",
        "lr_grid_search.fit(X_train, y_train)\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"â±ï¸ Search completed in {end_time - start_time:.2f} seconds\")\n",
        "print(f\"ğŸ† Best parameters: {lr_grid_search.best_params_}\")\n",
        "print(f\"ğŸ¯ Best cross-validation score: {lr_grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Test on holdout set\n",
        "lr_best = lr_grid_search.best_estimator_\n",
        "lr_y_pred = lr_best.predict(X_test)\n",
        "lr_accuracy = accuracy_score(y_test, lr_y_pred)\n",
        "print(f\"ğŸ“Š Test accuracy: {lr_accuracy:.4f}\")\n",
        "\n",
        "# Store results\n",
        "lr_results = {\n",
        "    'best_params': lr_grid_search.best_params_,\n",
        "    'best_cv_score': lr_grid_search.best_score_,\n",
        "    'test_accuracy': lr_accuracy,\n",
        "    'model': lr_best\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. ğŸ¯ SVM Hyperparameter Tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SVM Hyperparameter Tuning\n",
        "print(\"ğŸ¯ SVM Hyperparameter Tuning\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Define parameter grid for SVM\n",
        "svm_param_grid = {\n",
        "    'C': [0.1, 1, 10, 100, 1000],\n",
        "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
        "    'kernel': ['rbf', 'poly', 'sigmoid'],\n",
        "    'degree': [2, 3, 4, 5]  # Only used for poly kernel\n",
        "}\n",
        "\n",
        "# Initialize SVM\n",
        "svm = SVC(random_state=42, probability=True)\n",
        "\n",
        "# Use RandomizedSearchCV for SVM (can be slow)\n",
        "svm_random_search = RandomizedSearchCV(\n",
        "    estimator=svm,\n",
        "    param_distributions=svm_param_grid,\n",
        "    n_iter=30,  # Fewer iterations due to SVM being slower\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "print(\"ğŸ” Searching for best SVM parameters...\")\n",
        "start_time = time.time()\n",
        "svm_random_search.fit(X_train, y_train)\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"â±ï¸ Search completed in {end_time - start_time:.2f} seconds\")\n",
        "print(f\"ğŸ† Best parameters: {svm_random_search.best_params_}\")\n",
        "print(f\"ğŸ¯ Best cross-validation score: {svm_random_search.best_score_:.4f}\")\n",
        "\n",
        "# Test on holdout set\n",
        "svm_best = svm_random_search.best_estimator_\n",
        "svm_y_pred = svm_best.predict(X_test)\n",
        "svm_accuracy = accuracy_score(y_test, svm_y_pred)\n",
        "print(f\"ğŸ“Š Test accuracy: {svm_accuracy:.4f}\")\n",
        "\n",
        "# Store results\n",
        "svm_results = {\n",
        "    'best_params': svm_random_search.best_params_,\n",
        "    'best_cv_score': svm_random_search.best_score_,\n",
        "    'test_accuracy': svm_accuracy,\n",
        "    'model': svm_best\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. ğŸ“Š Model Comparison After Tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare all tuned models\n",
        "print(\"ğŸ“Š MODEL COMPARISON AFTER HYPERPARAMETER TUNING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create comparison DataFrame\n",
        "comparison_data = {\n",
        "    'Model': ['Random Forest', 'XGBoost', 'Logistic Regression', 'SVM'],\n",
        "    'Best CV Score': [\n",
        "        rf_results['best_cv_score'],\n",
        "        xgb_results['best_cv_score'],\n",
        "        lr_results['best_cv_score'],\n",
        "        svm_results['best_cv_score']\n",
        "    ],\n",
        "    'Test Accuracy': [\n",
        "        rf_results['test_accuracy'],\n",
        "        xgb_results['test_accuracy'],\n",
        "        lr_results['test_accuracy'],\n",
        "        svm_results['test_accuracy']\n",
        "    ]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "comparison_df = comparison_df.sort_values('Test Accuracy', ascending=False)\n",
        "\n",
        "print(\"\\nğŸ† RANKING BY TEST ACCURACY:\")\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Visualize comparison\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# CV Score comparison\n",
        "ax1.bar(comparison_df['Model'], comparison_df['Best CV Score'], color='skyblue', alpha=0.7)\n",
        "ax1.set_title('Cross-Validation Scores After Tuning', fontsize=14, fontweight='bold')\n",
        "ax1.set_ylabel('CV Score')\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Test Accuracy comparison\n",
        "ax2.bar(comparison_df['Model'], comparison_df['Test Accuracy'], color='lightcoral', alpha=0.7)\n",
        "ax2.set_title('Test Accuracy After Tuning', fontsize=14, fontweight='bold')\n",
        "ax2.set_ylabel('Test Accuracy')\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find best model\n",
        "best_model_name = comparison_df.iloc[0]['Model']\n",
        "best_accuracy = comparison_df.iloc[0]['Test Accuracy']\n",
        "\n",
        "print(f\"\\nğŸ¥‡ BEST MODEL: {best_model_name}\")\n",
        "print(f\"ğŸ¯ Best Test Accuracy: {best_accuracy:.4f}\")\n",
        "\n",
        "# Store all results\n",
        "all_tuned_results = {\n",
        "    'Random Forest': rf_results,\n",
        "    'XGBoost': xgb_results,\n",
        "    'Logistic Regression': lr_results,\n",
        "    'SVM': svm_results\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. ğŸ“ˆ Before vs After Tuning Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare with baseline models (from previous notebook)\n",
        "print(\"ğŸ“ˆ BEFORE vs AFTER HYPERPARAMETER TUNING COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Train baseline models for comparison\n",
        "baseline_models = {\n",
        "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
        "    'XGBoost': xgb.XGBClassifier(random_state=42, eval_metric='logloss'),\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "    'SVM': SVC(random_state=42, probability=True)\n",
        "}\n",
        "\n",
        "baseline_results = {}\n",
        "for name, model in baseline_models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    baseline_results[name] = accuracy\n",
        "    print(f\"ğŸ“Š {name} Baseline Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Create comparison DataFrame\n",
        "before_after_data = {\n",
        "    'Model': ['Random Forest', 'XGBoost', 'Logistic Regression', 'SVM'],\n",
        "    'Before Tuning': [\n",
        "        baseline_results['Random Forest'],\n",
        "        baseline_results['XGBoost'],\n",
        "        baseline_results['Logistic Regression'],\n",
        "        baseline_results['SVM']\n",
        "    ],\n",
        "    'After Tuning': [\n",
        "        rf_results['test_accuracy'],\n",
        "        xgb_results['test_accuracy'],\n",
        "        lr_results['test_accuracy'],\n",
        "        svm_results['test_accuracy']\n",
        "    ]\n",
        "}\n",
        "\n",
        "before_after_df = pd.DataFrame(before_after_data)\n",
        "before_after_df['Improvement'] = before_after_df['After Tuning'] - before_after_df['Before Tuning']\n",
        "\n",
        "print(\"\\nğŸ“Š BEFORE vs AFTER TUNING COMPARISON:\")\n",
        "print(before_after_df.to_string(index=False))\n",
        "\n",
        "# Visualize improvement\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "x = np.arange(len(before_after_df))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax.bar(x - width/2, before_after_df['Before Tuning'], width, label='Before Tuning', alpha=0.7, color='lightblue')\n",
        "bars2 = ax.bar(x + width/2, before_after_df['After Tuning'], width, label='After Tuning', alpha=0.7, color='lightcoral')\n",
        "\n",
        "ax.set_xlabel('Models')\n",
        "ax.set_ylabel('Test Accuracy')\n",
        "ax.set_title('Model Performance: Before vs After Hyperparameter Tuning', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(before_after_df['Model'], rotation=45)\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Add improvement annotations\n",
        "for i, (before, after, improvement) in enumerate(zip(before_after_df['Before Tuning'], \n",
        "                                                   before_after_df['After Tuning'], \n",
        "                                                   before_after_df['Improvement'])):\n",
        "    ax.annotate(f'+{improvement:.3f}', \n",
        "                xy=(i + width/2, after), \n",
        "                xytext=(0, 10), \n",
        "                textcoords='offset points',\n",
        "                ha='center', \n",
        "                fontweight='bold',\n",
        "                color='green' if improvement > 0 else 'red')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary of improvements\n",
        "total_improvement = before_after_df['Improvement'].sum()\n",
        "avg_improvement = before_after_df['Improvement'].mean()\n",
        "best_improvement = before_after_df['Improvement'].max()\n",
        "best_improved_model = before_after_df.loc[before_after_df['Improvement'].idxmax(), 'Model']\n",
        "\n",
        "print(f\"\\nğŸ“ˆ TUNING SUMMARY:\")\n",
        "print(f\"ğŸ¯ Total Improvement: {total_improvement:.4f}\")\n",
        "print(f\"ğŸ“Š Average Improvement: {avg_improvement:.4f}\")\n",
        "print(f\"ğŸ† Best Improvement: {best_improvement:.4f} ({best_improved_model})\")\n",
        "print(f\"ğŸ“ˆ Models with positive improvement: {(before_after_df['Improvement'] > 0).sum()}/4\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. ğŸ’¾ Save Best Models and Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save best models and results\n",
        "print(\"ğŸ’¾ SAVING BEST MODELS AND RESULTS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Create directories\n",
        "os.makedirs('../models/tuned_models', exist_ok=True)\n",
        "os.makedirs('../reports/results', exist_ok=True)\n",
        "\n",
        "# Save all tuned models\n",
        "print(\"ğŸ”§ Saving tuned models...\")\n",
        "for model_name, results in all_tuned_results.items():\n",
        "    model_path = f\"../models/tuned_models/{model_name.lower().replace(' ', '_')}_tuned.pkl\"\n",
        "    joblib.dump(results['model'], model_path)\n",
        "    print(f\"âœ… Saved {model_name} to {model_path}\")\n",
        "\n",
        "# Save hyperparameter results\n",
        "print(\"\\nğŸ“Š Saving hyperparameter results...\")\n",
        "tuning_results = {\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'best_models': {\n",
        "        model_name: {\n",
        "            'best_params': results['best_params'],\n",
        "            'best_cv_score': float(results['best_cv_score']),\n",
        "            'test_accuracy': float(results['test_accuracy'])\n",
        "        }\n",
        "        for model_name, results in all_tuned_results.items()\n",
        "    },\n",
        "    'comparison': {\n",
        "        'best_model': best_model_name,\n",
        "        'best_accuracy': float(best_accuracy),\n",
        "        'before_after_comparison': before_after_df.to_dict('records')\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save to JSON\n",
        "results_path = '../reports/results/hyperparameter_tuning_results.json'\n",
        "with open(results_path, 'w') as f:\n",
        "    json.dump(tuning_results, f, indent=2)\n",
        "print(f\"âœ… Saved tuning results to {results_path}\")\n",
        "\n",
        "# Save comparison DataFrame\n",
        "comparison_path = '../reports/results/model_comparison_after_tuning.csv'\n",
        "comparison_df.to_csv(comparison_path, index=False)\n",
        "print(f\"âœ… Saved model comparison to {comparison_path}\")\n",
        "\n",
        "# Save before/after comparison\n",
        "before_after_path = '../reports/results/before_after_tuning_comparison.csv'\n",
        "before_after_df.to_csv(before_after_path, index=False)\n",
        "print(f\"âœ… Saved before/after comparison to {before_after_path}\")\n",
        "\n",
        "print(f\"\\nğŸ¯ HYPERPARAMETER TUNING COMPLETED!\")\n",
        "print(f\"ğŸ† Best Model: {best_model_name}\")\n",
        "print(f\"ğŸ“Š Best Accuracy: {best_accuracy:.4f}\")\n",
        "print(f\"ğŸ“ˆ Total Improvement: {total_improvement:.4f}\")\n",
        "print(f\"ğŸ’¾ All results saved to ../reports/results/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. ğŸ“‹ Key Insights and Conclusions\n",
        "\n",
        "### ğŸ¯ **Hyperparameter Tuning Results Summary**\n",
        "\n",
        "#### **Best Performing Models:**\n",
        "1. **ğŸ¥‡ Best Model**: [Will be determined after running]\n",
        "2. **ğŸ“Š Best Accuracy**: [Will be determined after running]\n",
        "3. **ğŸ“ˆ Improvement**: [Will be determined after running]\n",
        "\n",
        "#### **Key Findings:**\n",
        "- **Random Forest**: Tuning typically improves performance by optimizing tree depth, number of estimators, and feature selection\n",
        "- **XGBoost**: Learning rate and regularization parameters are crucial for preventing overfitting\n",
        "- **Logistic Regression**: Regularization strength (C) and penalty type significantly affect performance\n",
        "- **SVM**: Kernel selection and gamma parameter are key for non-linear decision boundaries\n",
        "\n",
        "#### **Hyperparameter Importance:**\n",
        "- **Tree-based models** (RF, XGBoost): `max_depth`, `n_estimators`, `learning_rate`\n",
        "- **Linear models** (Logistic Regression): `C`, `penalty`, `solver`\n",
        "- **SVM**: `C`, `gamma`, `kernel`\n",
        "\n",
        "#### **Best Practices Applied:**\n",
        "- âœ… Used **RandomizedSearchCV** for faster exploration of large parameter spaces\n",
        "- âœ… Used **GridSearchCV** for smaller, discrete parameter spaces\n",
        "- âœ… Applied **5-fold cross-validation** for robust evaluation\n",
        "- âœ… Compared **before vs after** tuning to measure improvement\n",
        "- âœ… Saved all results for reproducibility\n",
        "\n",
        "#### **Next Steps:**\n",
        "1. **Ensemble Methods**: Combine best models using voting or stacking\n",
        "2. **Feature Engineering**: Further optimize features based on model insights\n",
        "3. **Final Submission**: Use best tuned model for Kaggle submission\n",
        "4. **Model Deployment**: Prepare best model for production use\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Importance Analysis for Best Model\n",
        "print(\"ğŸ” FEATURE IMPORTANCE ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Get the best model\n",
        "best_model = all_tuned_results[best_model_name]['model']\n",
        "\n",
        "# Check if model has feature_importances_ attribute\n",
        "if hasattr(best_model, 'feature_importances_'):\n",
        "    # Get feature importance\n",
        "    feature_importance = best_model.feature_importances_\n",
        "    feature_names = X.columns\n",
        "    \n",
        "    # Create DataFrame\n",
        "    importance_df = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'importance': feature_importance\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    \n",
        "    print(f\"ğŸ† Feature Importance for {best_model_name}:\")\n",
        "    print(importance_df.to_string(index=False))\n",
        "    \n",
        "    # Visualize feature importance\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.barh(importance_df['feature'], importance_df['importance'], color='skyblue', alpha=0.7)\n",
        "    plt.xlabel('Feature Importance')\n",
        "    plt.title(f'Feature Importance - {best_model_name}', fontsize=14, fontweight='bold')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "elif hasattr(best_model, 'coef_'):\n",
        "    # For linear models, show coefficients\n",
        "    coef = best_model.coef_[0]\n",
        "    feature_names = X.columns\n",
        "    \n",
        "    coef_df = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'coefficient': coef\n",
        "    }).sort_values('coefficient', key=abs, ascending=False)\n",
        "    \n",
        "    print(f\"ğŸ† Feature Coefficients for {best_model_name}:\")\n",
        "    print(coef_df.to_string(index=False))\n",
        "    \n",
        "    # Visualize coefficients\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    colors = ['red' if x < 0 else 'blue' for x in coef_df['coefficient']]\n",
        "    plt.barh(coef_df['feature'], coef_df['coefficient'], color=colors, alpha=0.7)\n",
        "    plt.xlabel('Coefficient Value')\n",
        "    plt.title(f'Feature Coefficients - {best_model_name}', fontsize=14, fontweight='bold')\n",
        "    plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "else:\n",
        "    print(f\"âš ï¸ {best_model_name} does not support feature importance analysis\")\n",
        "\n",
        "print(f\"\\nâœ… Hyperparameter tuning notebook completed successfully!\")\n",
        "print(f\"ğŸ“Š Ready for ensemble methods and final submission!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
