{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üó≥Ô∏è Titanic Survival Prediction - Ensemble Methods\n",
        "\n",
        "## üìä M·ª•c ti√™u\n",
        "- Implement ensemble methods ƒë·ªÉ c·∫£i thi·ªán performance\n",
        "- S·ª≠ d·ª•ng Voting Classifier v√† Stacking\n",
        "- So s√°nh performance c·ªßa ensemble vs individual models\n",
        "- T·∫°o final model cho submission\n",
        "\n",
        "## üìã N·ªôi dung\n",
        "1. **Data Preparation**\n",
        "2. **Voting Classifier**\n",
        "3. **Stacking Classifier**\n",
        "4. **Bagging Methods**\n",
        "5. **Ensemble Performance Comparison**\n",
        "6. **∆∞∆∞ an"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import VotingClassifier, StackingClassifier, BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import joblib\n",
        "import os\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"üìö Libraries imported successfully!\")\n",
        "print(\"üé® Visualization style set!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. üì• Data Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import preprocessing utilities\n",
        "import sys\n",
        "sys.path.append('../src')\n",
        "from data_preprocessing import load_data, preprocess_data, prepare_features, get_feature_columns\n",
        "\n",
        "# Load and preprocess data\n",
        "train_df, test_df = load_data('../data/raw/train.csv', '../data/raw/test.csv')\n",
        "processed_train_df, processed_test_df, label_encoders = preprocess_data(train_df, test_df)\n",
        "\n",
        "# Prepare features\n",
        "feature_columns = get_feature_columns()\n",
        "X = prepare_features(processed_train_df, feature_columns)\n",
        "y = processed_train_df['Survived']\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"üìä Training set shape: {X_train.shape}\")\n",
        "print(f\"üìä Test set shape: {X_test.shape}\")\n",
        "print(f\"üìä Feature columns: {feature_columns}\")\n",
        "\n",
        "# Display feature info\n",
        "print(f\"\\nüîç Feature Information:\")\n",
        "print(f\"Features: {list(X.columns)}\")\n",
        "print(f\"Target distribution: {y.value_counts().to_dict()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. üó≥Ô∏è Voting Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Voting Classifier Implementation\n",
        "print(\"üó≥Ô∏è VOTING CLASSIFIER\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create base models\n",
        "base_models = {\n",
        "    'rf': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    'xgb': xgb.XGBClassifier(random_state=42, eval_metric='logloss'),\n",
        "    'lgb': lgb.LGBMClassifier(random_state=42, verbose=-1),\n",
        "    'lr': LogisticRegression(random_state=42, max_iter=1000),\n",
        "    'svm': SVC(random_state=42, probability=True)\n",
        "}\n",
        "\n",
        "# Hard Voting Classifier\n",
        "print(\"üîß Creating Hard Voting Classifier...\")\n",
        "hard_voting = VotingClassifier(\n",
        "    estimators=list(base_models.items()),\n",
        "    voting='hard'\n",
        ")\n",
        "\n",
        "# Soft Voting Classifier\n",
        "print(\"üîß Creating Soft Voting Classifier...\")\n",
        "soft_voting = VotingClassifier(\n",
        "    estimators=list(base_models.items()),\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "# Train and evaluate hard voting\n",
        "print(\"\\nüìä Training Hard Voting Classifier...\")\n",
        "hard_voting.fit(X_train, y_train)\n",
        "hard_pred = hard_voting.predict(X_test)\n",
        "hard_accuracy = accuracy_score(y_test, hard_pred)\n",
        "\n",
        "print(f\"‚úÖ Hard Voting Accuracy: {hard_accuracy:.4f}\")\n",
        "\n",
        "# Train and evaluate soft voting\n",
        "print(\"\\nüìä Training Soft Voting Classifier...\")\n",
        "soft_voting.fit(X_train, y_train)\n",
        "soft_pred = soft_voting.predict(X_test)\n",
        "soft_accuracy = accuracy_score(y_test, soft_pred)\n",
        "\n",
        "print(f\"‚úÖ Soft Voting Accuracy: {soft_accuracy:.4f}\")\n",
        "\n",
        "# Store results\n",
        "voting_results = {\n",
        "    'Hard Voting': {\n",
        "        'model': hard_voting,\n",
        "        'accuracy': hard_accuracy,\n",
        "        'predictions': hard_pred\n",
        "    },\n",
        "    'Soft Voting': {\n",
        "        'model': soft_voting,\n",
        "        'accuracy': soft_accuracy,\n",
        "        'predictions': soft_pred\n",
        "    }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. üìö Stacking Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stacking Classifier Implementation\n",
        "print(\"üìö STACKING CLASSIFIER\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create stacking classifier with Logistic Regression as meta-learner\n",
        "print(\"üîß Creating Stacking Classifier...\")\n",
        "stacking_classifier = StackingClassifier(\n",
        "    estimators=list(base_models.items()),\n",
        "    final_estimator=LogisticRegression(random_state=42),\n",
        "    cv=5,\n",
        "    stack_method='predict_proba'\n",
        ")\n",
        "\n",
        "# Train and evaluate stacking\n",
        "print(\"\\nüìä Training Stacking Classifier...\")\n",
        "stacking_classifier.fit(X_train, y_train)\n",
        "stacking_pred = stacking_classifier.predict(X_test)\n",
        "stacking_accuracy = accuracy_score(y_test, stacking_pred)\n",
        "\n",
        "print(f\"‚úÖ Stacking Accuracy: {stacking_accuracy:.4f}\")\n",
        "\n",
        "# Create another stacking with Random Forest as meta-learner\n",
        "print(\"\\nüîß Creating Stacking with Random Forest Meta-learner...\")\n",
        "stacking_rf = StackingClassifier(\n",
        "    estimators=list(base_models.items()),\n",
        "    final_estimator=RandomForestClassifier(n_estimators=50, random_state=42),\n",
        "    cv=5,\n",
        "    stack_method='predict_proba'\n",
        ")\n",
        "\n",
        "# Train and evaluate stacking with RF\n",
        "print(\"\\nüìä Training Stacking with RF Meta-learner...\")\n",
        "stacking_rf.fit(X_train, y_train)\n",
        "stacking_rf_pred = stacking_rf.predict(X_test)\n",
        "stacking_rf_accuracy = accuracy_score(y_test, stacking_rf_pred)\n",
        "\n",
        "print(f\"‚úÖ Stacking (RF Meta) Accuracy: {stacking_rf_accuracy:.4f}\")\n",
        "\n",
        "# Store results\n",
        "stacking_results = {\n",
        "    'Stacking (LR Meta)': {\n",
        "        'model': stacking_classifier,\n",
        "        'accuracy': stacking_accuracy,\n",
        "        'predictions': stacking_pred\n",
        "    },\n",
        "    'Stacking (RF Meta)': {\n",
        "        'model': stacking_rf,\n",
        "        'accuracy': stacking_rf_accuracy,\n",
        "        'predictions': stacking_rf_pred\n",
        "    }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. üéí Bagging Methods\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bagging Methods Implementation\n",
        "print(\"üéí BAGGING METHODS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create bagging classifiers\n",
        "print(\"üîß Creating Bagging Classifiers...\")\n",
        "\n",
        "# Bagging with Decision Tree\n",
        "bagging_dt = BaggingClassifier(\n",
        "    base_estimator=RandomForestClassifier(n_estimators=10, random_state=42),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Bagging with Logistic Regression\n",
        "bagging_lr = BaggingClassifier(\n",
        "    base_estimator=LogisticRegression(random_state=42, max_iter=1000),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train and evaluate bagging with Decision Tree\n",
        "print(\"\\nüìä Training Bagging with Decision Tree...\")\n",
        "bagging_dt.fit(X_train, y_train)\n",
        "bagging_dt_pred = bagging_dt.predict(X_test)\n",
        "bagging_dt_accuracy = accuracy_score(y_test, bagging_dt_pred)\n",
        "\n",
        "print(f\"‚úÖ Bagging (DT) Accuracy: {bagging_dt_accuracy:.4f}\")\n",
        "\n",
        "# Train and evaluate bagging with Logistic Regression\n",
        "print(\"\\nüìä Training Bagging with Logistic Regression...\")\n",
        "bagging_lr.fit(X_train, y_train)\n",
        "bagging_lr_pred = bagging_lr.predict(X_test)\n",
        "bagging_lr_accuracy = accuracy_score(y_test, bagging_lr_pred)\n",
        "\n",
        "print(f\"‚úÖ Bagging (LR) Accuracy: {bagging_lr_accuracy:.4f}\")\n",
        "\n",
        "# Store results\n",
        "bagging_results = {\n",
        "    'Bagging (DT)': {\n",
        "        'model': bagging_dt,\n",
        "        'accuracy': bagging_dt_accuracy,\n",
        "        'predictions': bagging_dt_pred\n",
        "    },\n",
        "    'Bagging (LR)': {\n",
        "        'model': bagging_lr,\n",
        "        'accuracy': bagging_lr_accuracy,\n",
        "        'predictions': bagging_lr_pred\n",
        "    }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. üìä Ensemble Performance Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare all ensemble methods\n",
        "print(\"üìä ENSEMBLE PERFORMANCE COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Train individual base models for comparison\n",
        "print(\"üîß Training individual base models...\")\n",
        "individual_results = {}\n",
        "for name, model in base_models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, pred)\n",
        "    individual_results[name.upper()] = accuracy\n",
        "    print(f\"‚úÖ {name.upper()} Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Combine all results\n",
        "all_results = {\n",
        "    **individual_results,\n",
        "    **{name: result['accuracy'] for name, result in voting_results.items()},\n",
        "    **{name: result['accuracy'] for name, result in stacking_results.items()},\n",
        "    **{name: result['accuracy'] for name, result in bagging_results.items()}\n",
        "}\n",
        "\n",
        "# Create comparison DataFrame\n",
        "comparison_df = pd.DataFrame(list(all_results.items()), columns=['Model', 'Accuracy'])\n",
        "comparison_df = comparison_df.sort_values('Accuracy', ascending=False)\n",
        "\n",
        "print(f\"\\nüèÜ ENSEMBLE COMPARISON RESULTS:\")\n",
        "print(\"=\" * 60)\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Visualize comparison\n",
        "plt.figure(figsize=(15, 8))\n",
        "colors = ['lightcoral' if 'Voting' in model or 'Stacking' in model or 'Bagging' in model else 'skyblue' \n",
        "          for model in comparison_df['Model']]\n",
        "\n",
        "bars = plt.bar(range(len(comparison_df)), comparison_df['Accuracy'], color=colors, alpha=0.7)\n",
        "plt.xlabel('Models')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Ensemble Methods vs Individual Models Performance', fontsize=14, fontweight='bold')\n",
        "plt.xticks(range(len(comparison_df)), comparison_df['Model'], rotation=45, ha='right')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, (bar, acc) in enumerate(zip(bars, comparison_df['Accuracy'])):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n",
        "             f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find best ensemble method\n",
        "ensemble_models = [name for name in all_results.keys() \n",
        "                  if any(keyword in name for keyword in ['Voting', 'Stacking', 'Bagging'])]\n",
        "best_ensemble = max(ensemble_models, key=lambda x: all_results[x])\n",
        "best_ensemble_accuracy = all_results[best_ensemble]\n",
        "\n",
        "print(f\"\\nü•á BEST ENSEMBLE METHOD: {best_ensemble}\")\n",
        "print(f\"üéØ Best Ensemble Accuracy: {best_ensemble_accuracy:.4f}\")\n",
        "\n",
        "# Compare with best individual model\n",
        "best_individual = max(individual_results.keys(), key=lambda x: individual_results[x])\n",
        "best_individual_accuracy = individual_results[best_individual]\n",
        "\n",
        "improvement = best_ensemble_accuracy - best_individual_accuracy\n",
        "print(f\"üìà Improvement over best individual ({best_individual}): {improvement:.4f}\")\n",
        "\n",
        "# Store all ensemble results\n",
        "all_ensemble_results = {\n",
        "    'individual': individual_results,\n",
        "    'voting': voting_results,\n",
        "    'stacking': stacking_results,\n",
        "    'bagging': bagging_results,\n",
        "    'best_ensemble': best_ensemble,\n",
        "    'best_ensemble_accuracy': best_ensemble_accuracy\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. üíæ Save Best Ensemble Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save best ensemble model\n",
        "print(\"üíæ SAVING BEST ENSEMBLE MODEL\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create directories\n",
        "os.makedirs('../models/ensemble_models', exist_ok=True)\n",
        "os.makedirs('../reports/results', exist_ok=True)\n",
        "\n",
        "# Get best ensemble model\n",
        "if best_ensemble in voting_results:\n",
        "    best_model = voting_results[best_ensemble]['model']\n",
        "elif best_ensemble in stacking_results:\n",
        "    best_model = stacking_results[best_ensemble]['model']\n",
        "elif best_ensemble in bagging_results:\n",
        "    best_model = bagging_results[best_ensemble]['model']\n",
        "\n",
        "# Save best ensemble model\n",
        "best_model_path = f\"../models/ensemble_models/{best_ensemble.lower().replace(' ', '_')}_ensemble.pkl\"\n",
        "joblib.dump(best_model, best_model_path)\n",
        "print(f\"‚úÖ Saved best ensemble model to {best_model_path}\")\n",
        "\n",
        "# Save all ensemble models\n",
        "print(\"\\nüîß Saving all ensemble models...\")\n",
        "for category, results in [('voting', voting_results), ('stacking', stacking_results), ('bagging', bagging_results)]:\n",
        "    for name, result in results.items():\n",
        "        model_path = f\"../models/ensemble_models/{category}_{name.lower().replace(' ', '_')}.pkl\"\n",
        "        joblib.dump(result['model'], model_path)\n",
        "        print(f\"‚úÖ Saved {name} to {model_path}\")\n",
        "\n",
        "# Save ensemble results\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "ensemble_summary = {\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'best_ensemble': best_ensemble,\n",
        "    'best_ensemble_accuracy': float(best_ensemble_accuracy),\n",
        "    'improvement_over_individual': float(improvement),\n",
        "    'all_results': {k: float(v) for k, v in all_results.items()},\n",
        "    'comparison_summary': comparison_df.to_dict('records')\n",
        "}\n",
        "\n",
        "results_path = '../reports/results/ensemble_methods_results.json'\n",
        "with open(results_path, 'w') as f:\n",
        "    json.dump(ensemble_summary, f, indent=2)\n",
        "print(f\"‚úÖ Saved ensemble results to {results_path}\")\n",
        "\n",
        "# Save comparison DataFrame\n",
        "comparison_path = '../reports/results/ensemble_comparison.csv'\n",
        "comparison_df.to_csv(comparison_path, index=False)\n",
        "print(f\"‚úÖ Saved ensemble comparison to {comparison_path}\")\n",
        "\n",
        "print(f\"\\nüéØ ENSEMBLE METHODS COMPLETED!\")\n",
        "print(f\"üèÜ Best Ensemble: {best_ensemble}\")\n",
        "print(f\"üìä Best Accuracy: {best_ensemble_accuracy:.4f}\")\n",
        "print(f\"üìà Improvement: {improvement:.4f}\")\n",
        "print(f\"üíæ All models saved to ../models/ensemble_models/\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
