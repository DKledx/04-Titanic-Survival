{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ“„ Titanic Survival Prediction - Submission Preparation\n",
        "\n",
        "## ğŸ“Š Má»¥c tiÃªu\n",
        "- Chuáº©n bá»‹ final predictions cho Kaggle submission\n",
        "- Sá»­ dá»¥ng best model Ä‘Ã£ Ä‘Æ°á»£c tuned\n",
        "- Táº¡o submission file theo format Kaggle\n",
        "- Validate predictions trÆ°á»›c khi submit\n",
        "\n",
        "## ğŸ“‹ Ná»™i dung\n",
        "1. **Load Best Model**\n",
        "2. **Prepare Test Data**\n",
        "3. **Generate Predictions**\n",
        "4. **Create Submission File**\n",
        "5. **Validation & Final Checks**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import joblib\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"ğŸ“š Libraries imported successfully!\")\n",
        "print(\"ğŸ¨ Visualization style set!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. ğŸ“‚ Load Best Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best model from ensemble results\n",
        "print(\"ğŸ“‚ LOADING BEST MODEL\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Check for ensemble results\n",
        "ensemble_results_path = '../reports/results/ensemble_methods_results.json'\n",
        "tuning_results_path = '../reports/results/hyperparameter_tuning_results.json'\n",
        "\n",
        "best_model = None\n",
        "best_model_name = None\n",
        "model_source = None\n",
        "\n",
        "# Try to load ensemble results first\n",
        "if os.path.exists(ensemble_results_path):\n",
        "    print(\"ğŸ” Found ensemble results, loading best ensemble model...\")\n",
        "    with open(ensemble_results_path, 'r') as f:\n",
        "        ensemble_data = json.load(f)\n",
        "    \n",
        "    best_model_name = ensemble_data['best_ensemble']\n",
        "    best_model_path = f\"../models/ensemble_models/{best_model_name.lower().replace(' ', '_')}_ensemble.pkl\"\n",
        "    \n",
        "    if os.path.exists(best_model_path):\n",
        "        best_model = joblib.load(best_model_path)\n",
        "        model_source = \"ensemble\"\n",
        "        print(f\"âœ… Loaded best ensemble model: {best_model_name}\")\n",
        "    else:\n",
        "        print(\"âš ï¸ Ensemble model file not found, trying alternative paths...\")\n",
        "        # Try alternative naming\n",
        "        for file in os.listdir('../models/ensemble_models/'):\n",
        "            if best_model_name.lower().replace(' ', '_') in file.lower():\n",
        "                best_model = joblib.load(f\"../models/ensemble_models/{file}\")\n",
        "                model_source = \"ensemble\"\n",
        "                print(f\"âœ… Loaded ensemble model from: {file}\")\n",
        "                break\n",
        "\n",
        "# If no ensemble model, try tuned models\n",
        "if best_model is None and os.path.exists(tuning_results_path):\n",
        "    print(\"ğŸ” Loading best tuned model...\")\n",
        "    with open(tuning_results_path, 'r') as f:\n",
        "        tuning_data = json.load(f)\n",
        "    \n",
        "    best_model_name = tuning_data['comparison']['best_model']\n",
        "    best_model_path = f\"../models/tuned_models/{best_model_name.lower().replace(' ', '_')}_tuned.pkl\"\n",
        "    \n",
        "    if os.path.exists(best_model_path):\n",
        "        best_model = joblib.load(best_model_path)\n",
        "        model_source = \"tuned\"\n",
        "        print(f\"âœ… Loaded best tuned model: {best_model_name}\")\n",
        "    else:\n",
        "        print(\"âš ï¸ Tuned model file not found, trying alternative paths...\")\n",
        "        # Try alternative naming\n",
        "        for file in os.listdir('../models/tuned_models/'):\n",
        "            if best_model_name.lower().replace(' ', '_') in file.lower():\n",
        "                best_model = joblib.load(f\"../models/tuned_models/{file}\")\n",
        "                model_source = \"tuned\"\n",
        "                print(f\"âœ… Loaded tuned model from: {file}\")\n",
        "                break\n",
        "\n",
        "# If still no model, create a default one\n",
        "if best_model is None:\n",
        "    print(\"âš ï¸ No saved models found, creating default Random Forest model...\")\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    best_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    best_model_name = \"Random Forest (Default)\"\n",
        "    model_source = \"default\"\n",
        "\n",
        "print(f\"\\nğŸ“Š Model Information:\")\n",
        "print(f\"ğŸ† Best Model: {best_model_name}\")\n",
        "print(f\"ğŸ“‚ Source: {model_source}\")\n",
        "print(f\"ğŸ”§ Model Type: {type(best_model).__name__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. ğŸ“Š Prepare Test Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare test data for prediction\n",
        "print(\"ğŸ“Š PREPARING TEST DATA\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Import preprocessing utilities\n",
        "import sys\n",
        "sys.path.append('../src')\n",
        "from data_preprocessing import load_data, preprocess_data, prepare_features, get_feature_columns\n",
        "\n",
        "# Load and preprocess data\n",
        "train_df, test_df = load_data('../data/raw/train.csv', '../data/raw/test.csv')\n",
        "processed_train_df, processed_test_df, label_encoders = preprocess_data(train_df, test_df)\n",
        "\n",
        "# Prepare features for test set\n",
        "feature_columns = get_feature_columns()\n",
        "X_test_submission = prepare_features(processed_test_df, feature_columns)\n",
        "\n",
        "print(f\"ğŸ“Š Test set shape: {X_test_submission.shape}\")\n",
        "print(f\"ğŸ“Š Feature columns: {feature_columns}\")\n",
        "print(f\"ğŸ“Š Test set features: {list(X_test_submission.columns)}\")\n",
        "\n",
        "# Check for missing values\n",
        "missing_values = X_test_submission.isnull().sum()\n",
        "if missing_values.sum() > 0:\n",
        "    print(f\"\\nâš ï¸ Missing values found:\")\n",
        "    print(missing_values[missing_values > 0])\n",
        "    # Fill missing values with median for numeric columns\n",
        "    for col in X_test_submission.select_dtypes(include=[np.number]).columns:\n",
        "        if X_test_submission[col].isnull().sum() > 0:\n",
        "            median_val = X_test_submission[col].median()\n",
        "            X_test_submission[col].fillna(median_val, inplace=True)\n",
        "            print(f\"âœ… Filled missing values in {col} with median: {median_val}\")\n",
        "else:\n",
        "    print(\"âœ… No missing values found in test set\")\n",
        "\n",
        "# Get passenger IDs for submission\n",
        "passenger_ids = test_df['PassengerId'].values\n",
        "print(f\"\\nğŸ“‹ Passenger IDs: {len(passenger_ids)} passengers\")\n",
        "print(f\"ğŸ“‹ ID range: {passenger_ids.min()} - {passenger_ids.max()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. ğŸ¯ Generate Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate predictions using best model\n",
        "print(\"ğŸ¯ GENERATING PREDICTIONS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# If model is not trained, train it first\n",
        "if not hasattr(best_model, 'predict') or model_source == \"default\":\n",
        "    print(\"ğŸ”§ Training model on full training data...\")\n",
        "    X_train_full = prepare_features(processed_train_df, feature_columns)\n",
        "    y_train_full = processed_train_df['Survived']\n",
        "    \n",
        "    # Handle missing values in training data\n",
        "    for col in X_train_full.select_dtypes(include=[np.number]).columns:\n",
        "        if X_train_full[col].isnull().sum() > 0:\n",
        "            median_val = X_train_full[col].median()\n",
        "            X_train_full[col].fillna(median_val, inplace=True)\n",
        "    \n",
        "    best_model.fit(X_train_full, y_train_full)\n",
        "    print(\"âœ… Model trained successfully!\")\n",
        "\n",
        "# Generate predictions\n",
        "print(\"\\nğŸ”® Generating predictions...\")\n",
        "predictions = best_model.predict(X_test_submission)\n",
        "prediction_proba = best_model.predict_proba(X_test_submission)[:, 1] if hasattr(best_model, 'predict_proba') else None\n",
        "\n",
        "print(f\"âœ… Generated {len(predictions)} predictions\")\n",
        "print(f\"ğŸ“Š Prediction distribution:\")\n",
        "unique, counts = np.unique(predictions, return_counts=True)\n",
        "for val, count in zip(unique, counts):\n",
        "    print(f\"   Survived = {val}: {count} passengers ({count/len(predictions)*100:.1f}%)\")\n",
        "\n",
        "if prediction_proba is not None:\n",
        "    print(f\"\\nğŸ“Š Probability statistics:\")\n",
        "    print(f\"   Mean probability: {prediction_proba.mean():.4f}\")\n",
        "    print(f\"   Min probability: {prediction_proba.min():.4f}\")\n",
        "    print(f\"   Max probability: {prediction_proba.max():.4f}\")\n",
        "    print(f\"   Std probability: {prediction_proba.std():.4f}\")\n",
        "\n",
        "# Show some example predictions\n",
        "print(f\"\\nğŸ” Sample predictions:\")\n",
        "sample_indices = np.random.choice(len(predictions), min(10, len(predictions)), replace=False)\n",
        "for i, idx in enumerate(sample_indices):\n",
        "    prob_str = f\" (prob: {prediction_proba[idx]:.3f})\" if prediction_proba is not None else \"\"\n",
        "    print(f\"   Passenger {passenger_ids[idx]}: Survived = {predictions[idx]}{prob_str}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. ğŸ“„ Create Submission File\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create submission file\n",
        "print(\"ğŸ“„ CREATING SUBMISSION FILE\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create submissions directory\n",
        "os.makedirs('../submissions', exist_ok=True)\n",
        "\n",
        "# Create submission DataFrame\n",
        "submission_df = pd.DataFrame({\n",
        "    'PassengerId': passenger_ids,\n",
        "    'Survived': predictions\n",
        "})\n",
        "\n",
        "# Generate timestamp for unique filename\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "submission_filename = f\"../submissions/titanic_submission_{timestamp}.csv\"\n",
        "\n",
        "# Save submission file\n",
        "submission_df.to_csv(submission_filename, index=False)\n",
        "print(f\"âœ… Submission file created: {submission_filename}\")\n",
        "\n",
        "# Also create a latest submission file\n",
        "latest_submission_filename = \"../submissions/titanic_submission_latest.csv\"\n",
        "submission_df.to_csv(latest_submission_filename, index=False)\n",
        "print(f\"âœ… Latest submission file created: {latest_submission_filename}\")\n",
        "\n",
        "# Display submission file info\n",
        "print(f\"\\nğŸ“Š Submission File Information:\")\n",
        "print(f\"ğŸ“ File: {submission_filename}\")\n",
        "print(f\"ğŸ“ Shape: {submission_df.shape}\")\n",
        "print(f\"ğŸ“‹ Columns: {list(submission_df.columns)}\")\n",
        "\n",
        "# Show first few rows\n",
        "print(f\"\\nğŸ” First 10 rows of submission:\")\n",
        "print(submission_df.head(10).to_string(index=False))\n",
        "\n",
        "# Show last few rows\n",
        "print(f\"\\nğŸ” Last 10 rows of submission:\")\n",
        "print(submission_df.tail(10).to_string(index=False))\n",
        "\n",
        "# Validate submission format\n",
        "print(f\"\\nâœ… Submission Validation:\")\n",
        "print(f\"   âœ“ Correct number of predictions: {len(predictions) == len(passenger_ids)}\")\n",
        "print(f\"   âœ“ All predictions are 0 or 1: {all(pred in [0, 1] for pred in predictions)}\")\n",
        "print(f\"   âœ“ No missing values: {not submission_df.isnull().any().any()}\")\n",
        "print(f\"   âœ“ PassengerId range: {passenger_ids.min()} - {passenger_ids.max()}\")\n",
        "print(f\"   âœ“ File size: {os.path.getsize(submission_filename)} bytes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. âœ… Validation & Final Checks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final validation and checks\n",
        "print(\"âœ… VALIDATION & FINAL CHECKS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Compare with sample submission\n",
        "sample_submission_path = '../data/raw/gender_submission.csv'\n",
        "if os.path.exists(sample_submission_path):\n",
        "    print(\"ğŸ” Comparing with sample submission...\")\n",
        "    sample_submission = pd.read_csv(sample_submission_path)\n",
        "    \n",
        "    print(f\"ğŸ“Š Sample submission shape: {sample_submission.shape}\")\n",
        "    print(f\"ğŸ“Š Our submission shape: {submission_df.shape}\")\n",
        "    \n",
        "    # Check if shapes match\n",
        "    if sample_submission.shape == submission_df.shape:\n",
        "        print(\"âœ… Submission shapes match!\")\n",
        "    else:\n",
        "        print(\"âš ï¸ Submission shapes don't match!\")\n",
        "    \n",
        "    # Check column names\n",
        "    if list(sample_submission.columns) == list(submission_df.columns):\n",
        "        print(\"âœ… Column names match!\")\n",
        "    else:\n",
        "        print(\"âš ï¸ Column names don't match!\")\n",
        "        print(f\"   Sample: {list(sample_submission.columns)}\")\n",
        "        print(f\"   Ours: {list(submission_df.columns)}\")\n",
        "\n",
        "# Create summary report\n",
        "print(f\"\\nğŸ“‹ SUBMISSION SUMMARY REPORT\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "submission_summary = {\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'model_info': {\n",
        "        'name': best_model_name,\n",
        "        'type': type(best_model).__name__,\n",
        "        'source': model_source\n",
        "    },\n",
        "    'data_info': {\n",
        "        'test_set_size': len(predictions),\n",
        "        'features_used': len(feature_columns),\n",
        "        'feature_list': feature_columns\n",
        "    },\n",
        "    'predictions': {\n",
        "        'total_predictions': len(predictions),\n",
        "        'survived_0': int(np.sum(predictions == 0)),\n",
        "        'survived_1': int(np.sum(predictions == 1)),\n",
        "        'survival_rate': float(np.mean(predictions))\n",
        "    },\n",
        "    'files': {\n",
        "        'submission_file': submission_filename,\n",
        "        'latest_file': latest_submission_filename\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save summary report\n",
        "summary_path = '../reports/results/submission_summary.json'\n",
        "os.makedirs('../reports/results', exist_ok=True)\n",
        "with open(summary_path, 'w') as f:\n",
        "    json.dump(submission_summary, f, indent=2)\n",
        "print(f\"âœ… Summary report saved: {summary_path}\")\n",
        "\n",
        "# Display summary\n",
        "print(f\"\\nğŸ“Š FINAL SUBMISSION SUMMARY:\")\n",
        "print(f\"ğŸ† Model: {best_model_name}\")\n",
        "print(f\"ğŸ“‚ Source: {model_source}\")\n",
        "print(f\"ğŸ“Š Test Set: {len(predictions)} passengers\")\n",
        "print(f\"ğŸ”§ Features: {len(feature_columns)} features\")\n",
        "print(f\"ğŸ“ˆ Survival Rate: {np.mean(predictions):.1%}\")\n",
        "print(f\"ğŸ“„ Files Created:\")\n",
        "print(f\"   â€¢ {submission_filename}\")\n",
        "print(f\"   â€¢ {latest_submission_filename}\")\n",
        "print(f\"   â€¢ {summary_path}\")\n",
        "\n",
        "# Final validation checklist\n",
        "print(f\"\\nâœ… FINAL VALIDATION CHECKLIST:\")\n",
        "print(f\"   âœ“ Model loaded successfully\")\n",
        "print(f\"   âœ“ Test data prepared correctly\")\n",
        "print(f\"   âœ“ Predictions generated\")\n",
        "print(f\"   âœ“ Submission file created\")\n",
        "print(f\"   âœ“ File format validated\")\n",
        "print(f\"   âœ“ No missing values\")\n",
        "print(f\"   âœ“ All predictions are 0 or 1\")\n",
        "print(f\"   âœ“ Correct number of predictions\")\n",
        "\n",
        "print(f\"\\nğŸ¯ SUBMISSION READY FOR KAGGLE!\")\n",
        "print(f\"ğŸ“¤ Upload file: {latest_submission_filename}\")\n",
        "print(f\"ğŸ† Good luck with your submission!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
